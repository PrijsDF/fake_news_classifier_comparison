{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, Sentence\n",
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "\n",
    "# FLAIR: https://github.com/zalandoresearch/flair\n",
    "# LLPOF: https://github.com/thiagorainmaker77/liar_dataset\n",
    "# 0 == true class, 1 == fake class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Says the Annies List political group supports third-trimester abortions on demand.'\n",
      " 'When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.'\n",
      " 'Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"'\n",
      " 'Health care reform legislation is likely to mandate free sex change surgeries.'\n",
      " 'The economic turnaround started at the end of my term.'\n",
      " 'The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.'\n",
      " 'Jim Dunnam has not lived in the district he represents for years now.'\n",
      " \"I'm the only person on this stage who has worked actively just last year passing, along with Russ Feingold, some of the toughest ethics reform since Watergate.\"\n",
      " 'However, it took $19.5 million in Oregon Lottery funds for the Port of Newport to eventually land the new NOAA Marine Operations Center-Pacific.'\n",
      " 'Says GOP primary opponents Glenn Grothman and Joe Leibham cast a compromise vote that cost $788 million in higher electricity costs.'\n",
      " 'For the first time in history, the share of the national popular vote margin is smaller than the Latino vote margin.'\n",
      " 'Since 2000, nearly 12 million Americans have slipped out of the middle class and into poverty.'\n",
      " 'When Mitt Romney was governor of Massachusetts, we didnt just slow the rate of growth of our government, we actually cut it.'\n",
      " 'The economy bled $24 billion due to the government shutdown.'\n",
      " 'Most of the (Affordable Care Act) has already in some sense been waived or otherwise suspended.'\n",
      " 'In this last election in November, ... 63 percent of the American people chose not to vote, ... 80 percent of young people, (and) 75 percent of low-income workers chose not to vote.'\n",
      " \"McCain opposed a requirement that the government buy American-made motorcycles. And he said all buy-American provisions were quote 'disgraceful.' \"\n",
      " 'U.S. Rep. Ron Kind, D-Wis., and his fellow Democrats went on a spending spree and now their credit card is maxed out'\n",
      " 'Water rates in Manila, Philippines, were raised up to 845 percent when a subsidiary of the World Bank became a partial owner.']\n"
     ]
    }
   ],
   "source": [
    "# Importing and formatting the data\n",
    "raw_data = []\n",
    "raw_labels = []\n",
    "with open(\"data/data.tsv\", encoding=\"utf8\") as input_file:\n",
    "    try:\n",
    "        for row in csv.reader(input_file, delimiter=\"\\t\"):\n",
    "            if row[1] in [\"true\", \"mostly-true\", \"half-true\"]:\n",
    "                train_label = 0\n",
    "            else:\n",
    "                train_label = 1\n",
    "\n",
    "            train_statement = row[2]\n",
    "            #train_statement = re.sub(r'[^\\w\\s]', '', train_statement)\n",
    "\n",
    "            raw_data.append(train_statement)\n",
    "            raw_labels.append(train_label)\n",
    "    except:\n",
    "        print(\"A train line was skipped due to an error\")\n",
    "\n",
    "raw_data = np.asarray(raw_data)\n",
    "raw_labels = np.asarray(raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "\n",
      "Embedding 19 sentences\n",
      "Remaining time: 0.1 min [#############                                                                   ][array([], dtype=float32)]\n",
      "Remaining time: 0.0 min [############################################################################    ][array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32), array([], dtype=float32)]\n",
      "\n",
      "Embedding completed. Total duration: 0.1 min\n"
     ]
    }
   ],
   "source": [
    "# Getting embeddings\n",
    "raw_embeddings = []\n",
    "\n",
    "# initialize the word embeddings\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([flair_embedding_forward])\n",
    "\n",
    "total_start_time = time.time()\n",
    "timestamps = []\n",
    "total_embeddings = len(raw_data)\n",
    "progress_bar_width = 80\n",
    "progress_iterations = total_embeddings\n",
    "\n",
    "# Iterate over the batches and create embeddings\n",
    "for i in range(0, total_embeddings):\n",
    "    if i == 0:\n",
    "        print(f\"\\nEmbedding {total_embeddings} sentences\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training embeddings\n",
    "    document_embeddings.embed(Sentence(raw_data[i]))\n",
    "    raw_embeddings += [np.array(\n",
    "        Sentence(raw_data[i]).get_embedding().detach())]\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    timestamps.append(elapsed_time)\n",
    "    \n",
    "    remaining = sum(timestamps) / len(timestamps)\n",
    "    remaining = remaining * (total_embeddings - i + 1)\n",
    "    remaining = round(remaining / 60, 1)\n",
    "\n",
    "    hash_count = round(progress_bar_width * (i/progress_iterations))\n",
    "    hashes = '#' * hash_count\n",
    "    spaces = ' ' * (progress_bar_width - hash_count)\n",
    "\n",
    "    print(f\"\\rRemaining time: {remaining} min [{hashes}{spaces}]\",\n",
    "          end='')\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "raw_embeddings = np.asarray(raw_embeddings)\n",
    "\n",
    "print(f\"\\nEmbedding completed. Total duration:\",\n",
    "      round(total_elapsed_time / 60, 1),\n",
    "      \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(true, predicted):\n",
    "    decimals = 3\n",
    "\n",
    "    accuracy = round(metrics.accuracy_score(true, predicted), decimals)\n",
    "    precision = round(metrics.precision_score(true, predicted), decimals)\n",
    "    recall = round(metrics.recall_score(true, predicted), decimals)\n",
    "    f1 = round(metrics.f1_score(true, predicted), decimals)\n",
    "\n",
    "    confusion = metrics.confusion_matrix(true, predicted, labels=[1, 0])\n",
    "    return [[\"accuracy: \", accuracy],\n",
    "            [\"precision: \", precision],\n",
    "            [\"recall: \", recall],\n",
    "            [\"f1: \", f1],\n",
    "            [\"confusion matrix: \", confusion]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10 predictions for each treatment\n",
      "Results are stored in resultss.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-ad296ec2d585>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtrain_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mtest_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "total_start_time = time.time()\n",
    "timestamps = []\n",
    "progress_bar_width = 80\n",
    "progress_iterations = sample_size\n",
    "save_location = 'resultss.csv'\n",
    "\n",
    "for i in range(0, sample_size):\n",
    "    if i == 0:\n",
    "        print(f\"Starting {sample_size} predictions for each treatment\")\n",
    "        print(f\"Results are stored in {save_location}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fetching train and test splits\n",
    "    shufflesplit = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=round(len(raw_data)*0.2))\n",
    "    \n",
    "    samples = shufflesplit.split(raw_embeddings, raw_labels)\n",
    "   \n",
    "    for train_index, test_index in samples:\n",
    "        train_embeddings = raw_embeddings[train_index]\n",
    "        test_embeddings = raw_embeddings[test_index]\n",
    "        \n",
    "        train_data = np.array(raw_data[train_index])\n",
    "        test_data = np.array(raw_data[test_index])\n",
    "        \n",
    "        train_labels = raw_labels[train_index]\n",
    "        test_labels = raw_labels[test_index]\n",
    "\n",
    "\n",
    "    # Create TF-IDF DTM for baselines\n",
    "    # Train\n",
    "    count_vect = CountVectorizer()\n",
    "    train_counts = count_vect.fit_transform(train_data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "    dense_train_tfidf = train_tfidf.toarray()\n",
    "\n",
    "    # Test\n",
    "    test_counts = count_vect.transform(test_data)\n",
    "    test_tfidf = tfidf_transformer.transform(test_counts)\n",
    "    dense_test_tfidf = test_tfidf.toarray()\n",
    "\n",
    "    # Baselines + embeddings\n",
    "    # Classification tree\n",
    "    tree_md5 = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "    tree_md5.fit(train_tfidf, train_labels)\n",
    "    tree_md5_eval = evaluate(test_labels, tree_md5.predict(test_tfidf))\n",
    "    #print(\"\\nTree md5:\", *tree_md5_eval, sep=\"\\n\")\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(dense_train_tfidf, train_labels)\n",
    "    gnb_eval = evaluate(test_labels, gnb.predict(dense_test_tfidf))\n",
    "    #print(\"\\nGaussian NB:\", *gnb_eval, sep=\"\\n\")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lsr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lsr.fit(train_tfidf, train_labels)\n",
    "    lsr_eval = evaluate(test_labels, lsr.predict(test_tfidf))\n",
    "    #print(\"\\nLogistic Regression:\", *lsr_eval, sep=\"\\n\")\n",
    "\n",
    "    # SVM Linear\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(train_tfidf, train_labels)\n",
    "    svm_eval = evaluate(test_labels, svm.predict(test_tfidf))\n",
    "    #print(\"\\nSVM linear:\", *svm_eval, sep=\"\\n\")\n",
    "\n",
    "    with open(save_location, 'a', newline='\\n') as newFile:\n",
    "        newFileWriter = csv.writer(newFile)\n",
    "        newFileWriter.writerow(['Tree', \n",
    "                                tree_md5_eval[0][1],\n",
    "                                tree_md5_eval[1][1],\n",
    "                                tree_md5_eval[2][1],\n",
    "                                tree_md5_eval[3][1]])\n",
    "        newFileWriter.writerow(['NaiveBayes', \n",
    "                                gnb_eval[0][1],\n",
    "                                gnb_eval[1][1],\n",
    "                                gnb_eval[2][1],\n",
    "                                gnb_eval[3][1]])\n",
    "        newFileWriter.writerow(['LR', \n",
    "                                lsr_eval[0][1],\n",
    "                                lsr_eval[1][1],\n",
    "                                lsr_eval[2][1],\n",
    "                                lsr_eval[3][1]])\n",
    "        newFileWriter.writerow(['SVM', \n",
    "                                svm_eval[0][1],\n",
    "                                svm_eval[1][1],\n",
    "                                svm_eval[2][1],\n",
    "                                svm_eval[3][1]])\n",
    "\n",
    "    # Baselines + embeddings\n",
    "    # Classification Tree\n",
    "    tree_md5 = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "    tree_md5.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    tree_md5_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                             tree_md5.predict(test_embeddings))\n",
    "    #print('Tree md5:', *tree_md5_eval, sep=\"\\n\")\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    gnb_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                        gnb.predict(test_embeddings))\n",
    "    #print('\\nGaussian NB:', *gnb_eval, sep=\"\\n\")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lsr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lsr.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    lsr_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                        lsr.predict(test_embeddings))\n",
    "    #print('\\nLogistic Regression:', *lsr_eval, sep=\"\\n\")\n",
    "\n",
    "    # SVM Linear\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    svm_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                        svm.predict(test_embeddings))\n",
    "    #print('\\nSVM linear:', *svm_eval, sep=\"\\n\")\n",
    "\n",
    "    with open(save_location, 'a', newline='\\n') as newFile:\n",
    "        newFileWriter = csv.writer(newFile)\n",
    "        newFileWriter.writerow(['Tree + embeddings', \n",
    "                                tree_md5_eval[0][1],\n",
    "                                tree_md5_eval[1][1],\n",
    "                                tree_md5_eval[2][1],\n",
    "                                tree_md5_eval[3][1]])\n",
    "        newFileWriter.writerow(['NaiveBayes + embeddings', \n",
    "                                gnb_eval[0][1],\n",
    "                                gnb_eval[1][1],\n",
    "                                gnb_eval[2][1],\n",
    "                                gnb_eval[3][1]])\n",
    "        newFileWriter.writerow(['LR + embeddings', \n",
    "                                lsr_eval[0][1],\n",
    "                                lsr_eval[1][1],\n",
    "                                lsr_eval[2][1],\n",
    "                                lsr_eval[3][1]])\n",
    "        newFileWriter.writerow(['SVM + embeddings', \n",
    "                                svm_eval[0][1],\n",
    "                                svm_eval[1][1],\n",
    "                                svm_eval[2][1],\n",
    "                                svm_eval[3][1]])\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    timestamps.append(elapsed_time)\n",
    "    \n",
    "    remaining = sum(timestamps) / len(timestamps)\n",
    "    remaining = remaining * (progress_iterations - i + 1)\n",
    "    remaining = round(remaining / 60, 1)\n",
    "\n",
    "    hash_count = round(progress_bar_width * ((i+1) / progress_iterations))\n",
    "    hashes = '#' * hash_count\n",
    "    spaces = ' ' * (progress_bar_width - hash_count)\n",
    "\n",
    "    print(f\"\\rRemaining time: {remaining} min [{hashes}{spaces}]\",\n",
    "          end='')\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "\n",
    "print(\"\")\n",
    "print(\"\\rPredictions completed. Total duration:\",\n",
    "      round(total_elapsed_time / 60, 1),\n",
    "      \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
