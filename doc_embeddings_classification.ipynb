{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, Sentence\n",
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "\n",
    "# FLAIR: https://github.com/zalandoresearch/flair\n",
    "# LLPOF: https://github.com/thiagorainmaker77/liar_dataset\n",
    "# 0 == true class, 1 == fake class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and formatting the data\n",
    "raw_data = []\n",
    "raw_labels = []\n",
    "with open(\"data/data.tsv\", encoding=\"utf8\") as input_file:\n",
    "    try:\n",
    "        for row in csv.reader(input_file, delimiter=\"\\t\"):\n",
    "            if row[1] in [\"true\", \"mostly-true\", \"half-true\"]:\n",
    "                train_label = 0\n",
    "            else:\n",
    "                train_label = 1\n",
    "\n",
    "            train_statement = row[2]\n",
    "            #train_statement = re.sub(r'[^\\w\\s]', '', train_statement)\n",
    "\n",
    "            raw_data.append(train_statement)\n",
    "            raw_labels.append(train_label)\n",
    "    except:\n",
    "        print(\"A train line was skipped due to an error\")\n",
    "\n",
    "raw_data = np.asarray(raw_data)\n",
    "raw_labels = np.asarray(raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding 12791 sentences\n",
      "Remaining time: 0.0 min [################################################################################]]\n",
      "Embedding completed. Total duration: 71.2 min\n"
     ]
    }
   ],
   "source": [
    "# Getting embeddings\n",
    "raw_embeddings = []\n",
    "\n",
    "# initialize the word embeddings\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "# initialize the document embeddings, mode = mean\n",
    "document_embeddings = DocumentPoolEmbeddings([flair_embedding_forward])\n",
    "\n",
    "total_start_time = time.time()\n",
    "timestamps = []\n",
    "total_embeddings = len(raw_data)\n",
    "progress_bar_width = 80\n",
    "progress_iterations = total_embeddings\n",
    "\n",
    "# Iterate over the batches and create embeddings\n",
    "for i in range(0, total_embeddings):\n",
    "    if i == 0:\n",
    "        print(f\"\\nEmbedding {total_embeddings} sentences\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training embeddings\n",
    "    sent = Sentence(raw_data[i])\n",
    "    document_embeddings.embed(sent)\n",
    "    raw_embeddings += [np.array(sent.get_embedding().detach())]\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    timestamps.append(elapsed_time)\n",
    "    \n",
    "    remaining = sum(timestamps) / len(timestamps)\n",
    "    remaining = remaining * (total_embeddings - i + 1)\n",
    "    remaining = round(remaining / 60, 1)\n",
    "\n",
    "    hash_count = round(progress_bar_width * (i/progress_iterations))\n",
    "    hashes = '#' * hash_count\n",
    "    spaces = ' ' * (progress_bar_width - hash_count)\n",
    "\n",
    "    print(f\"\\rRemaining time: {remaining} min [{hashes}{spaces}]\",\n",
    "          end='')\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "raw_embeddings = np.asarray(raw_embeddings)\n",
    "\n",
    "print(f\"\\nEmbedding completed. Total duration:\",\n",
    "      round(total_elapsed_time / 60, 1),\n",
    "      \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(true, predicted):\n",
    "    decimals = 3\n",
    "\n",
    "    accuracy = round(metrics.accuracy_score(true, predicted), decimals)\n",
    "    precision = round(metrics.precision_score(true, predicted), decimals)\n",
    "    recall = round(metrics.recall_score(true, predicted), decimals)\n",
    "    f1 = round(metrics.f1_score(true, predicted), decimals)\n",
    "\n",
    "    confusion = metrics.confusion_matrix(true, predicted, labels=[1, 0])\n",
    "    return [[\"accuracy: \", accuracy],\n",
    "            [\"precision: \", precision],\n",
    "            [\"recall: \", recall],\n",
    "            [\"f1: \", f1],\n",
    "            [\"confusion matrix: \", confusion]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10 predictions for each treatment\n",
      "Results are stored in results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5544491\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Remaining time: 43.7 min [########                                                                        ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5544491\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Remaining time: 39.5 min [################                                                                ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5544491\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Remaining time: 35.8 min [########################                                                        ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5544491\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining time: 28.0 min [########################################                                        ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5544491\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Remaining time: 24.0 min [################################################                                ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\5544491\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining time: 11.9 min [########################################################################        ]"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "total_start_time = time.time()\n",
    "timestamps = []\n",
    "progress_bar_width = 80\n",
    "progress_iterations = sample_size\n",
    "save_location = 'results.csv'\n",
    "\n",
    "for i in range(0, sample_size):\n",
    "    if i == 0:\n",
    "        print(f\"Starting {sample_size} predictions for each treatment\")\n",
    "        print(f\"Results are stored in {save_location}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fetching train and test splits\n",
    "    shufflesplit = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=round(len(raw_data)*0.2))\n",
    "    \n",
    "    samples = shufflesplit.split(raw_embeddings, raw_labels)\n",
    "   \n",
    "    for train_index, test_index in samples:\n",
    "        train_embeddings = raw_embeddings[train_index]\n",
    "        test_embeddings = raw_embeddings[test_index]\n",
    "        \n",
    "        train_data = np.array(raw_data[train_index])\n",
    "        test_data = np.array(raw_data[test_index])\n",
    "        \n",
    "        train_labels = raw_labels[train_index]\n",
    "        test_labels = raw_labels[test_index]\n",
    "\n",
    "\n",
    "    # Create TF-IDF DTM for baselines\n",
    "    # Train\n",
    "    count_vect = CountVectorizer()\n",
    "    train_counts = count_vect.fit_transform(train_data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "    dense_train_tfidf = train_tfidf.toarray()\n",
    "\n",
    "    # Test\n",
    "    test_counts = count_vect.transform(test_data)\n",
    "    test_tfidf = tfidf_transformer.transform(test_counts)\n",
    "    dense_test_tfidf = test_tfidf.toarray()\n",
    "\n",
    "    # Baselines + embeddings\n",
    "    # Classification tree\n",
    "    tree_md5 = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "    tree_md5.fit(train_tfidf, train_labels)\n",
    "    tree_md5_eval = evaluate(test_labels, tree_md5.predict(test_tfidf))\n",
    "    #print(\"\\nTree md5:\", *tree_md5_eval, sep=\"\\n\")\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(dense_train_tfidf, train_labels)\n",
    "    gnb_eval = evaluate(test_labels, gnb.predict(dense_test_tfidf))\n",
    "    #print(\"\\nGaussian NB:\", *gnb_eval, sep=\"\\n\")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lsr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lsr.fit(train_tfidf, train_labels)\n",
    "    lsr_eval = evaluate(test_labels, lsr.predict(test_tfidf))\n",
    "    #print(\"\\nLogistic Regression:\", *lsr_eval, sep=\"\\n\")\n",
    "\n",
    "    # SVM Linear\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(train_tfidf, train_labels)\n",
    "    svm_eval = evaluate(test_labels, svm.predict(test_tfidf))\n",
    "    #print(\"\\nSVM linear:\", *svm_eval, sep=\"\\n\")\n",
    "\n",
    "    with open(save_location, 'a', newline='\\n') as newFile:\n",
    "        newFileWriter = csv.writer(newFile)\n",
    "        newFileWriter.writerow(['Tree', \n",
    "                                tree_md5_eval[0][1],\n",
    "                                tree_md5_eval[1][1],\n",
    "                                tree_md5_eval[2][1],\n",
    "                                tree_md5_eval[3][1]])\n",
    "        newFileWriter.writerow(['NaiveBayes', \n",
    "                                gnb_eval[0][1],\n",
    "                                gnb_eval[1][1],\n",
    "                                gnb_eval[2][1],\n",
    "                                gnb_eval[3][1]])\n",
    "        newFileWriter.writerow(['LR', \n",
    "                                lsr_eval[0][1],\n",
    "                                lsr_eval[1][1],\n",
    "                                lsr_eval[2][1],\n",
    "                                lsr_eval[3][1]])\n",
    "        newFileWriter.writerow(['SVM', \n",
    "                                svm_eval[0][1],\n",
    "                                svm_eval[1][1],\n",
    "                                svm_eval[2][1],\n",
    "                                svm_eval[3][1]])\n",
    "\n",
    "    # Baselines + embeddings\n",
    "    # Classification Tree\n",
    "    tree_md5 = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "    tree_md5.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    tree_md5_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                             tree_md5.predict(test_embeddings))\n",
    "    #print('Tree md5:', *tree_md5_eval, sep=\"\\n\")\n",
    "\n",
    "    # Gaussian NB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    gnb_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                        gnb.predict(test_embeddings))\n",
    "    #print('\\nGaussian NB:', *gnb_eval, sep=\"\\n\")\n",
    "\n",
    "    # Logistic Regression\n",
    "    lsr = LogisticRegression(solver=\"lbfgs\")\n",
    "    lsr.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    lsr_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                        lsr.predict(test_embeddings))\n",
    "    #print('\\nLogistic Regression:', *lsr_eval, sep=\"\\n\")\n",
    "\n",
    "    # SVM Linear\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(train_embeddings, train_labels[:len(train_embeddings)])\n",
    "    svm_eval = evaluate(test_labels[:len(test_embeddings)],\n",
    "                        svm.predict(test_embeddings))\n",
    "    #print('\\nSVM linear:', *svm_eval, sep=\"\\n\")\n",
    "\n",
    "    with open(save_location, 'a', newline='\\n') as newFile:\n",
    "        newFileWriter = csv.writer(newFile)\n",
    "        newFileWriter.writerow(['Tree + embeddings', \n",
    "                                tree_md5_eval[0][1],\n",
    "                                tree_md5_eval[1][1],\n",
    "                                tree_md5_eval[2][1],\n",
    "                                tree_md5_eval[3][1]])\n",
    "        newFileWriter.writerow(['NaiveBayes + embeddings', \n",
    "                                gnb_eval[0][1],\n",
    "                                gnb_eval[1][1],\n",
    "                                gnb_eval[2][1],\n",
    "                                gnb_eval[3][1]])\n",
    "        newFileWriter.writerow(['LR + embeddings', \n",
    "                                lsr_eval[0][1],\n",
    "                                lsr_eval[1][1],\n",
    "                                lsr_eval[2][1],\n",
    "                                lsr_eval[3][1]])\n",
    "        newFileWriter.writerow(['SVM + embeddings', \n",
    "                                svm_eval[0][1],\n",
    "                                svm_eval[1][1],\n",
    "                                svm_eval[2][1],\n",
    "                                svm_eval[3][1]])\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    timestamps.append(elapsed_time)\n",
    "    \n",
    "    remaining = sum(timestamps) / len(timestamps)\n",
    "    remaining = remaining * (progress_iterations - i + 1)\n",
    "    remaining = round(remaining / 60, 1)\n",
    "\n",
    "    hash_count = round(progress_bar_width * ((i+1) / progress_iterations))\n",
    "    hashes = '#' * hash_count\n",
    "    spaces = ' ' * (progress_bar_width - hash_count)\n",
    "\n",
    "    print(f\"\\rRemaining time: {remaining} min [{hashes}{spaces}]\",\n",
    "          end='')\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "\n",
    "print(\"\")\n",
    "print(\"\\rPredictions completed. Total duration:\",\n",
    "      round(total_elapsed_time / 60, 1),\n",
    "      \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
